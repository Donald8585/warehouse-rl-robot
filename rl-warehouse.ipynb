{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## ðŸ¤– Warehouse RL Robot (BFS+RL Hybrid)\n","metadata":{}},{"cell_type":"code","source":"!pip install gymnasium torch imageio numpy matplotlib -q","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gymnasium as gym\nfrom gymnasium import spaces\nimport numpy as np\nfrom collections import deque\nimport os\nimport imageio\n\nprint(\"ðŸ“¦ Importing SB3...\")\nfrom stable_baselines3 import DQN\nfrom stable_baselines3.her.her_replay_buffer import HerReplayBuffer\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# 1. WAREHOUSE ENVIRONMENT\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nclass WarehouseEnv(gym.Env):\n    metadata = {\"render_modes\": [\"rgb_array\"], \"render_fps\": 10}\n\n    def __init__(\n        self,\n        grid_size=20,\n        obstacle_density=0.3,\n        min_distance=None,\n        max_distance=None,\n        render_mode='rgb_array',\n        fixed_start_goal=None,\n    ):\n        super().__init__()\n        self.grid_size = grid_size\n        self.max_steps = 200\n        self.render_mode = render_mode\n        self.obstacle_density = obstacle_density\n        self.min_distance = min_distance if min_distance is not None else grid_size // 2\n        self.max_distance = max_distance if max_distance is not None else np.sqrt(2) * grid_size\n        self.fixed_start_goal = fixed_start_goal\n\n        self.observation_space = spaces.Dict({\n            'observation': spaces.Box(0, grid_size, shape=(2,), dtype=np.float32),\n            'achieved_goal': spaces.Box(0, grid_size, shape=(2,), dtype=np.float32),\n            'desired_goal': spaces.Box(0, grid_size, shape=(2,), dtype=np.float32),\n        })\n        self.action_space = spaces.Discrete(4)\n        \n        self._rng = np.random.default_rng()\n        self.obstacles = None\n        self._reset_world()\n\n    def _sample_start_goal(self):\n        if self.fixed_start_goal is not None:\n            agent_pos, goal_pos = self.fixed_start_goal\n            return np.array(agent_pos, dtype=np.int32), np.array(goal_pos, dtype=np.int32)\n\n        for _ in range(500):\n            agent = self._rng.integers(3, self.grid_size - 3, size=2, dtype=np.int32)\n            goal = self._rng.integers(3, self.grid_size - 3, size=2, dtype=np.int32)\n            dist = np.linalg.norm(goal - agent)\n            if (\n                dist >= self.min_distance\n                and dist <= self.max_distance\n                and not np.array_equal(agent, goal)\n            ):\n                return agent, goal\n\n        return np.array([3, 3], dtype=np.int32), np.array([self.grid_size-4, self.grid_size-4], dtype=np.int32)\n\n    def _reset_world(self):\n        self.agent_pos, self.goal_pos = self._sample_start_goal()\n        \n        self.obstacles = np.zeros((self.grid_size, self.grid_size), dtype=np.int32)\n        self.obstacles[0, :] = self.obstacles[-1, :] = 1\n        self.obstacles[:, 0] = self.obstacles[:, -1] = 1\n        \n        if self.obstacle_density >= 0.20:\n            col = self.grid_size // 2\n            for row in range(3, self.grid_size-3):\n                if row % 3 != 0:\n                    self.obstacles[row, col] = 1\n        \n        if self.obstacle_density >= 0.35:\n            for col in [self.grid_size // 3, 2 * self.grid_size // 3]:\n                for row in range(3, self.grid_size-3):\n                    if row % 3 != 1:\n                        self.obstacles[row, col] = 1\n        \n        free_cells = np.argwhere(self.obstacles == 0)\n        n_extra = int(len(free_cells) * self.obstacle_density * 0.4)\n        if n_extra > 0:\n            idx = self._rng.choice(len(free_cells), size=n_extra, replace=False)\n            for (x, y) in free_cells[idx]:\n                self.obstacles[x, y] = 1\n        \n        # FIXED: Larger safety zone (7x7)\n        for dx in range(-3, 4):\n            for dy in range(-3, 4):\n                for pos in [self.agent_pos, self.goal_pos]:\n                    p = pos + [dx, dy]\n                    if 0 <= p[0] < self.grid_size and 0 <= p[1] < self.grid_size:\n                        self.obstacles[tuple(p)] = 0\n        \n        self.steps = 0\n\n    def reset(self, seed=None, options=None):\n        if seed is not None:\n            self._rng = np.random.default_rng(seed)\n        self._reset_world()\n        return self._get_obs(), {}\n\n    def _get_obs(self):\n        return {\n            'observation': self.agent_pos.astype(np.float32),\n            'achieved_goal': self.agent_pos.astype(np.float32),\n            'desired_goal': self.goal_pos.astype(np.float32),\n        }\n\n    def step(self, action):\n        self.steps += 1\n        moves = [np.array([-1,0]), np.array([0,1]), np.array([1,0]), np.array([0,-1])]\n        new_pos = self.agent_pos + moves[action]\n        \n        if (\n            0 <= new_pos[0] < self.grid_size\n            and 0 <= new_pos[1] < self.grid_size\n            and self.obstacles[tuple(new_pos)] == 0\n        ):\n            self.agent_pos = new_pos\n        \n        obs = self._get_obs()\n        reward = self.compute_reward(obs['achieved_goal'], obs['desired_goal'], None)\n        terminated = (reward == 0.0)\n        truncated = (self.steps >= self.max_steps)\n        return obs, reward, terminated, truncated, {}\n\n    def compute_reward(self, achieved_goal, desired_goal, info):\n        if achieved_goal.ndim == 1:\n            dist = np.linalg.norm(achieved_goal - desired_goal)\n            return np.array(0.0 if dist < 1.0 else -1.0, dtype=np.float32)\n        else:\n            dist = np.linalg.norm(achieved_goal - desired_goal, axis=1)\n            return np.array([0.0 if d < 1.0 else -1.0 for d in dist], dtype=np.float32)\n\n    def render(self):\n        if self.render_mode != 'rgb_array':\n            return None\n        size = 400\n        img = np.ones((size, size, 3), dtype=np.uint8) * 240\n        cell = size // self.grid_size\n        \n        for x in range(self.grid_size):\n            for y in range(self.grid_size):\n                if self.obstacles[x, y]:\n                    img[x*cell:(x+1)*cell, y*cell:(y+1)*cell] = [139, 90, 43]\n        \n        gx, gy = self.goal_pos\n        margin = cell // 4\n        img[gx*cell+margin:(gx+1)*cell-margin, gy*cell+margin:(gy+1)*cell-margin] = [0, 200, 0]\n        \n        ax, ay = self.agent_pos\n        img[ax*cell:(ax+1)*cell, ay*cell:(ay+1)*cell] = [30, 144, 255]\n        return img\n\nprint(\"âœ… WarehouseEnv loaded!\")\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# 2. BFS PATHFINDING\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\ndef bfs_search(grid_size, obstacles, start, goal):\n    directions = [(-1,0), (0,1), (1,0), (0,-1)]\n    queue = deque([(start, [])])\n    visited = set([tuple(start)])\n    \n    while queue:\n        pos, path = queue.popleft()\n        if np.array_equal(pos, goal):\n            return path + [pos]\n        \n        for dx, dy in directions:\n            nx, ny = pos + np.array([dx, dy])\n            if (\n                0 <= nx < grid_size\n                and 0 <= ny < grid_size\n                and obstacles[int(nx), int(ny)] == 0\n                and (int(nx), int(ny)) not in visited\n            ):\n                visited.add((int(nx), int(ny)))\n                queue.append((np.array([nx, ny]), path + [pos]))\n    return None\n\ndef get_action(current_pos, next_pos):\n    dx, dy = next_pos - current_pos\n    if dx == -1: return 0\n    if dy == 1: return 1\n    if dx == 1: return 2\n    if dy == -1: return 3\n    return 0\n\nprint(\"âœ… BFS solver loaded!\")\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# 3. CURRICULUM LEARNING (SINGLE ENV - NO MULTIPROCESSING)\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"ðŸŽ“ CURRICULUM LEARNING: EASY â†’ MEDIUM â†’ HARD\")\nprint(\"=\"*70)\n\n# Stage 1: EASY\nprint(\"\\nðŸ“š Stage 1/3: Training on EASY (density=0.10, distance=5-8)\")\nenv_easy = WarehouseEnv(grid_size=20, obstacle_density=0.10, min_distance=5, max_distance=8)\n\nmodel = DQN(\n    \"MultiInputPolicy\",\n    env_easy,\n    replay_buffer_class=HerReplayBuffer,\n    replay_buffer_kwargs=dict(n_sampled_goal=4, goal_selection_strategy='future'),\n    learning_rate=1e-3,\n    buffer_size=100000,\n    learning_starts=2000,\n    batch_size=256,\n    verbose=1,\n    device='cuda'\n)\n\nprint(\"â³ Training 100,000 steps on EASY...\")\nmodel.learn(total_timesteps=100000)\nprint(\"âœ… Stage 1 complete!\")\n\n# Stage 2: MEDIUM (just update env parameters - no new env object)\nprint(\"\\nðŸ“š Stage 2/3: Training on MEDIUM (density=0.25, distance=8-12)\")\nenv_easy.obstacle_density = 0.25\nenv_easy.min_distance = 8\nenv_easy.max_distance = 12\n\nprint(\"â³ Training 100,000 steps on MEDIUM...\")\nmodel.learn(total_timesteps=100000, reset_num_timesteps=False)\nprint(\"âœ… Stage 2 complete!\")\n\n# Stage 3: HARD\nprint(\"\\nðŸ“š Stage 3/3: Training on HARD (density=0.40, distance=12-16)\")\nenv_easy.obstacle_density = 0.40\nenv_easy.min_distance = 12\nenv_easy.max_distance = 16\n\nprint(\"â³ Training 100,000 steps on HARD...\")\nmodel.learn(total_timesteps=100000, reset_num_timesteps=False)\nprint(\"âœ… Stage 3 complete!\")\n\nenv_easy.close()\n\nmodel.save(\"warehouse_dqn_her_curriculum\")\nprint(\"\\nðŸŽ‰ CURRICULUM TRAINING COMPLETE! (300k total steps)\")\nprint(\"â±ï¸  Estimated time: ~45-60 min on Kaggle T4\")\nprint(\"=\"*70)\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# 4. SIDE-BY-SIDE EVAL\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\ndef side_by_side_eval(model, num_runs=5):\n    os.makedirs('videos_comparison', exist_ok=True)\n\n    difficulties = {\n        'tutorial': (20, 0.00, 3, 5),\n        'easy':     (20, 0.10, 5, 8),\n        'medium':   (20, 0.25, 8, 12),\n        'hard':     (20, 0.40, 12, 16),\n        'expert':   (20, 0.55, 14, 18),\n        'nightmare':(20, 0.70, 16, 22),\n    }\n\n    results = {'bfs': [], 'rl': []}\n    \n    print(\"\\nðŸŽ¬ SIDE-BY-SIDE: BFS (BLUE) vs RL (GREEN)...\")\n    \n    for diff_name, (size, density, min_dist, max_dist) in difficulties.items():\n        print(f\"\\nðŸ”„ {diff_name.upper()}\")\n        \n        for run in range(1, num_runs + 1):\n            tmp_env = WarehouseEnv(\n                grid_size=size,\n                obstacle_density=density,\n                min_distance=min_dist,\n                max_distance=max_dist,\n            )\n            obs_tmp, _ = tmp_env.reset()\n            shared_start = tmp_env.agent_pos.copy()\n            shared_goal = tmp_env.goal_pos.copy()\n            tmp_env.close()\n\n            fixed_pair = (shared_start, shared_goal)\n\n            # BFS run\n            env_bfs = WarehouseEnv(\n                grid_size=size,\n                obstacle_density=density,\n                min_distance=min_dist,\n                max_distance=max_dist,\n                fixed_start_goal=fixed_pair,\n            )\n            obs_bfs, _ = env_bfs.reset()\n            frames_bfs = [env_bfs.render()]\n\n            path = bfs_search(env_bfs.grid_size, env_bfs.obstacles, obs_bfs['observation'], obs_bfs['desired_goal'])\n            if path is not None:\n                for next_pos in path[1:]:\n                    action = get_action(obs_bfs['observation'], next_pos)\n                    obs_bfs, _, term, trunc, _ = env_bfs.step(action)\n                    frames_bfs.append(env_bfs.render())\n                    if term or trunc:\n                        break\n\n            for _ in range(8):\n                frames_bfs.append(env_bfs.render())\n            steps_bfs = env_bfs.steps\n            success_bfs = path is not None\n            env_bfs.close()\n\n            # RL run\n            env_rl = WarehouseEnv(\n                grid_size=size,\n                obstacle_density=density,\n                min_distance=min_dist,\n                max_distance=max_dist,\n                fixed_start_goal=fixed_pair,\n            )\n            obs_rl, _ = env_rl.reset()\n            frames_rl = [env_rl.render()]\n            last_reward = -1.0\n\n            for _ in range(200):\n                action, _ = model.predict(obs_rl, deterministic=True)\n                obs_rl, reward, term, trunc, _ = env_rl.step(action)\n                frames_rl.append(env_rl.render())\n                last_reward = reward\n                if term or trunc:\n                    break\n\n            for _ in range(8):\n                frames_rl.append(frames_rl[-1])\n            steps_rl = env_rl.steps\n            success_rl = (last_reward == 0.0)\n            env_rl.close()\n\n            # Split-screen\n            max_frames = max(len(frames_bfs), len(frames_rl))\n            frames_bfs += [frames_bfs[-1]] * (max_frames - len(frames_bfs))\n            frames_rl += [frames_rl[-1]] * (max_frames - len(frames_rl))\n\n            split_frames = []\n            for i in range(max_frames):\n                left = frames_bfs[i].copy()\n                right = frames_rl[i].copy()\n                left[:, :, 2] = np.minimum(255, left[:, :, 2] + 50)\n                right[:, :, 1] = np.minimum(255, right[:, :, 1] + 80)\n                split_frames.append(np.hstack([left, right]))\n\n            video_path = f'videos_comparison/{diff_name}_run{run:02d}_BFSvsRL.mp4'\n            imageio.mimsave(video_path, split_frames, fps=10)\n            print(\n                f\"   ðŸŽ¥ {diff_name} Run {run}: \"\n                f\"BFS={steps_bfs}s {'âœ…' if success_bfs else 'âŒ'} | \"\n                f\"RL={steps_rl}s {'âœ…' if success_rl else 'âŒ'} â†’ {video_path}\"\n            )\n\n            results['bfs'].append({'diff': diff_name, 'steps': steps_bfs, 'success': success_bfs})\n            results['rl'].append({'diff': diff_name, 'steps': steps_rl, 'success': success_rl})\n    \n    print(\"\\nðŸŽ¯ Videos in /kaggle/working/videos_comparison/\")\n    return results\n\nresults = side_by_side_eval(model)\n\n# Summary\nprint(\"\\n\" + \"=\"*70)\nprint(\"ðŸ“Š SUCCESS RATE SUMMARY\")\nprint(\"=\"*70)\nfor diff in ['tutorial', 'easy', 'medium', 'hard', 'expert', 'nightmare']:\n    bfs_success = sum(1 for r in results['bfs'] if r['diff'] == diff and r['success'])\n    rl_success = sum(1 for r in results['rl'] if r['diff'] == diff and r['success'])\n    print(f\"{diff.upper():12s} | BFS: {bfs_success}/5 ({bfs_success*20}%) | RL: {rl_success}/5 ({rl_success*20}%)\")\n\nprint(\"\\nðŸš€ COMPLETE! 30 MP4s ready\")\nprint(\"â±ï¸  Total time: ~45-60 min on Kaggle T4\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}