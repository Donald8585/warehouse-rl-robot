# ğŸ¤– Warehouse Robot Navigation | DQN + HER + Curriculum Learning

[![Live Demo](https://img.shields.io/badge/ğŸ¤—%20Hugging%20Face-Live%20Demo-blue)](https://huggingface.co/spaces/Donald8585/warehouse-robot-navigation)
[![GitHub](https://img.shields.io/badge/GitHub-Repository-181717?logo=github)](https://github.com/Donald8585/warehouse-rl-robot)
[![Python](https://img.shields.io/badge/Python-3.10+-3776AB?logo=python)](https://www.python.org/)
[![Stable-Baselines3](https://img.shields.io/badge/Stable--Baselines3-DQN-orange)](https://stable-baselines3.readthedocs.io/)

Deep Reinforcement Learning agent using **DQN with Hindsight Experience Replay (HER)** trained through **curriculum learning** across 6 difficulty levels. Side-by-side video comparison demonstrates BFS optimal pathfinding vs. trained RL agent performance.

<p align="center">
  <img src="https://img.shields.io/badge/Success%20Rate-100%25%20Easy-brightgreen" alt="Easy Success"/>
  <img src="https://img.shields.io/badge/Success%20Rate-20%25%20Medium-yellow" alt="Medium Success"/>
  <img src="https://img.shields.io/badge/Training-300K%20Steps-blue" alt="Training Steps"/>
</p>

---

## ğŸ¯ Key Features

- âœ… **Curriculum Learning**: 300k training steps across 3 progressive stages (Easy â†’ Medium â†’ Hard)
- âœ… **DQN with Hindsight Experience Replay**: Efficient learning in sparse reward environments
- âœ… **6 Difficulty Levels**: 0-70% obstacle density, variable goal distances (3-22 grid units)
- âœ… **Real-time Video Comparison**: BFS optimal path (LEFT/BLUE) vs. RL agent (RIGHT/GREEN)
- âœ… **Path Validation System**: BFS pre-check ensures all maps are solvable
- âœ… **Interactive Streamlit UI**: 30 comparison videos (5 runs Ã— 6 difficulties)
- âœ… **Deployed on HuggingFace Spaces**: Free CPU-tier hosting with zero operational cost

---

## ğŸ“Š Performance Results

| Difficulty | Obstacle Density | Distance | RL Success Rate | BFS Success Rate |
|-----------|------------------|----------|-----------------|------------------|
| ğŸŸ¢ Tutorial | 0% | 3-5 | **100%** âœ… | 100% |
| ğŸ”µ Easy | 10% | 5-8 | **100%** âœ… | 100% |
| ğŸŸ¡ Medium | 25% | 8-12 | **20%** âš ï¸ | 100% |
| ğŸŸ  Hard | 40% | 12-16 | **0%** âŒ | 100% |
| ğŸ”´ Expert | 55% | 14-18 | **0%** âŒ | 100% |
| âš« Nightmare | 70% | 16-22 | **0%** âŒ | 100% |

**Key Insights:**
- âœ… **Perfect performance on simpler tasks** - Agent mastered Tutorial and Easy levels
- âš ï¸ **Limited generalization** - Struggles with higher obstacle density (40%+)
- ğŸ“‰ **Sparse rewards challenge** - Only 1/5 Medium runs succeeded
- ğŸ¯ **BFS baseline demonstrates optimal performance** across all difficulties

---

## ğŸ› ï¸ Tech Stack

**Reinforcement Learning:**
- Stable Baselines3 (DQN + HER implementation)
- Gymnasium (custom grid environment)
- NumPy (grid-based simulation)

**Training Infrastructure:**
- Kaggle T4 GPU (~1 hour training time)
- 300k total timesteps (100k per stage)
- Episode limit: 200 steps

**Deployment:**
- Streamlit (interactive web UI)
- ImageIO (video generation)
- HuggingFace Spaces (free CPU hosting)

---

## ğŸ“ Project Structure

```
warehouse-rl-robot/
â”œâ”€â”€ rl-warehouse.ipynb          # ğŸ“ Main Kaggle notebook (training + evaluation)
â”œâ”€â”€ web_demo/
â”‚   â”œâ”€â”€ app.py                  # Streamlit web interface
â”‚   â””â”€â”€ videos/                 # 30 comparison videos (5 runs Ã— 6 levels)
â”‚       â”œâ”€â”€ tutorial_run01_BFSvsRL.mp4
â”‚       â”œâ”€â”€ easy_run01_BFSvsRL.mp4
â”‚       â””â”€â”€ ...
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

**Workflow:**
1. **Train on Kaggle** â†’ `rl-warehouse.ipynb` (DQN+HER curriculum learning)
2. **Generate videos** â†’ Output 30 comparison videos
3. **Deploy to HF Spaces** â†’ `web_demo/` folder with Streamlit UI

---

## ğŸš€ Quick Start

### 1. View Live Demo

ğŸ‘‰ **[Try it now on HuggingFace Spaces](https://huggingface.co/spaces/Donald8585/warehouse-robot-navigation)** ğŸ‘ˆ

### 2. Train Your Own Agent (Kaggle)

**Open the notebook:**
```
rl-warehouse.ipynb
```

**What it does:**
- ğŸ—ï¸ Defines custom Gymnasium environment (20Ã—20 grid)
- ğŸ§  Trains DQN+HER agent with curriculum learning
- ğŸ“¹ Generates side-by-side BFS vs RL comparison videos
- ğŸ’¾ Outputs 30 videos for web demo

**Training time:** ~1 hour on Kaggle T4 GPU (free tier)

### 3. Run Web Demo Locally

```bash
cd web_demo
streamlit run app.py
```

Select difficulty level and run number to watch BFS vs RL comparisons!

---

## ğŸ® Environment Details

### State Space (4 features)
- Agent position: `(x, y)` coordinates
- Goal position: `(goal_x, goal_y)` coordinates
- Grid size: 20Ã—20 cells

### Action Space (Discrete: 4 actions)
- `0`: Move Up
- `1`: Move Right
- `2`: Move Down
- `3`: Move Left

### Reward Structure
- **Goal reached**: `+0` (sparse reward)
- **All other steps**: `-1` (time penalty)
- Episode terminates at goal or after 200 steps

### Difficulty Configuration

```python
DIFFICULTIES = {
    "tutorial": {"obstacle_pct": 0.0, "min_dist": 3, "max_dist": 5},
    "easy": {"obstacle_pct": 0.10, "min_dist": 5, "max_dist": 8},
    "medium": {"obstacle_pct": 0.25, "min_dist": 8, "max_dist": 12},
    "hard": {"obstacle_pct": 0.40, "min_dist": 12, "max_dist": 16},
    "expert": {"obstacle_pct": 0.55, "min_dist": 14, "max_dist": 18},
    "nightmare": {"obstacle_pct": 0.70, "min_dist": 16, "max_dist": 22}
}
```

**Safety Zones:** 7Ã—7 clearance around start and goal positions to ensure navigability.

---

## ğŸ’¡ Technical Highlights

### 1. Hindsight Experience Replay (HER)
```python
model = DQN(
    "MlpPolicy",
    env,
    replay_buffer_class=HerReplayBuffer,
    replay_buffer_kwargs=dict(
        n_sampled_goal=4,
        goal_selection_strategy="future"
    )
)
```
HER enables learning from failed episodes by treating achieved states as alternative goals.

### 2. Curriculum Learning
Progressive training across difficulty levels prevents catastrophic forgetting:
- **Stage 1**: Master easy navigation (0-10% obstacles) - 100k steps
- **Stage 2**: Handle moderate complexity (25% obstacles) - 100k steps
- **Stage 3**: Attempt high difficulty (40% obstacles) - 100k steps

### 3. Path Validation
```python
def is_solvable(grid, start, goal):
    return bfs_path_exists(grid, start, goal)
```
BFS pre-validation ensures all generated maps have valid solutions.

### 4. Side-by-Side Comparison
- **LEFT (Blue tint)**: BFS optimal algorithm
- **RIGHT (Green tint)**: Trained RL agent
- Synchronized obstacle maps for fair evaluation

---

## ğŸ”¬ Challenges & Insights

### âœ… What Worked
- Curriculum learning enabled 100% success on Tutorial/Easy levels
- HER significantly improved sample efficiency with sparse rewards
- Path validation prevented impossible maze configurations
- Side-by-side visualization clearly demonstrates algorithm differences

### âš ï¸ Limitations
- **Poor generalization to unseen complexity** - Agent fails on 40%+ obstacle density
- **Sample inefficiency at higher difficulties** - 300k steps insufficient for Hard+ levels
- **Sparse reward challenge** - Agent struggles to explore effectively in complex mazes

### ğŸ”® Future Improvements
- [ ] Implement reward shaping (distance to goal, collision penalties)
- [ ] Extend curriculum learning to 500k+ steps
- [ ] Add exploration bonuses (curiosity-driven learning)
- [ ] Test alternative algorithms (PPO, SAC, Rainbow DQN)
- [ ] Implement continuous action space for smoother navigation

---

## ğŸ“ Key Concepts Demonstrated

- âœ… Deep Reinforcement Learning (DQN)
- âœ… Hindsight Experience Replay (HER)
- âœ… Curriculum Learning
- âœ… Custom Gymnasium Environment
- âœ… Sparse Reward Environments
- âœ… Algorithm Comparison (RL vs Classical)
- âœ… Video Visualization
- âœ… Production Deployment (HuggingFace Spaces)
- âœ… Honest Performance Reporting

---

## ğŸŒ Live Demo

**Try it yourself:** [https://huggingface.co/spaces/Donald8585/warehouse-robot-navigation](https://huggingface.co/spaces/Donald8585/warehouse-robot-navigation)

**Features:**
- Select difficulty level (Tutorial â†’ Nightmare)
- Choose run number (1-5 different random seeds)
- Watch side-by-side BFS vs RL comparison
- View real-time success metrics

**Embed in your site:**
```html
<iframe 
    src="https://donald8585-warehouse-robot-navigation.hf.space" 
    width="100%" 
    height="800"
    style="border: 1px solid #ddd; border-radius: 8px;">
</iframe>
```

---

## ğŸ“ Citation

If you use this project in your research or work, please cite:

```bibtex
@software{so2026warehouse,
  author = {So, Chit Wai Alfred},
  title = {Warehouse Robot Navigation with DQN+HER+Curriculum Learning},
  year = {2026},
  url = {https://github.com/Donald8585/warehouse-rl-robot}
}
```

---

## ğŸ“œ License

MIT License - feel free to use for learning and portfolio projects.

---

## ğŸ”— Connect

**Portfolio:** [https://alfredso.com/portfolio](https://alfredso.com/portfolio)  
**GitHub:** [https://github.com/Donald8585](https://github.com/Donald8585)  
**LinkedIn:** [https://linkedin.com/in/alfred-so](https://linkedin.com/in/alfred-so)  
**Kaggle:** [https://www.kaggle.com/sword4949](https://www.kaggle.com/sword4949)

---

<p align="center">
  <i>Part of Alfred So's ML portfolio showcasing production-ready RL systems</i><br>
  <i>MSc Data Science & AI | Hong Kong</i>
</p>

<p align="center">
  Made with â¤ï¸ and ğŸ¤– by <a href="https://github.com/Donald8585">Alfred So</a>
</p>
